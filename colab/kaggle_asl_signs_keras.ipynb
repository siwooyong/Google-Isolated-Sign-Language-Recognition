{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU43IlEiFTWj"
      },
      "source": [
        "# keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEu9voccFdH6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# GPU 메모리 할당 방식 변경\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # 현재 GPU에 할당되어 있는 메모리 양을 동적으로 할당하도록 설정\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYpYwszTFXCO"
      },
      "source": [
        "## library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgt5gt_iFerh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPEt31n2FvK4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from transformers import TFRobertaPreLayerNormModel, RobertaPreLayerNormConfig\n",
        "\n",
        "#import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import random\n",
        "import os\n",
        "import json \n",
        "import math\n",
        "import gc\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import AdamW, Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5vrK7McFfNH"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbQjtmYhFxFE"
      },
      "outputs": [],
      "source": [
        "def load_relevant_data_subset_with_imputation(args, pq_path):\n",
        "  data_columns = ['x', 'y', 'z']\n",
        "  data = pd.read_parquet(pq_path, columns=data_columns)\n",
        "  data.replace(np.nan, 0, inplace=True)\n",
        "  n_frames = int(len(data) / args.rows_per_frame)\n",
        "  data = data.values.reshape(n_frames, args.rows_per_frame, len(data_columns))\n",
        "  return data.astype(np.float32)\n",
        "\n",
        "def load_relevant_data_subset(args, pq_path):\n",
        "  data_columns = ['x', 'y', 'z']\n",
        "  data = pd.read_parquet(pq_path, columns=data_columns)\n",
        "  n_frames = int(len(data) / args.rows_per_frame)\n",
        "  data = data.values.reshape(n_frames, args.rows_per_frame, len(data_columns))\n",
        "  return data.astype(np.float32)\n",
        "\n",
        "def read_dict(args, file_path):\n",
        "  path = os.path.expanduser(file_path)\n",
        "  with open(path, \"r\") as f:\n",
        "    dic = json.load(f)\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JTuuQTFFgSU"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkU-RQCLFzIw"
      },
      "outputs": [],
      "source": [
        "class CustomConfig():\n",
        "\n",
        "  # training\n",
        "  seed = 42\n",
        "  batch_size = 128\n",
        "  num_workers = 12\n",
        "  device = 'cuda'\n",
        "  folder = 'result'\n",
        "  lr = 1e-3\n",
        "  epoch_n = 40\n",
        "  rows_per_frame = 75\n",
        "  warmup_ratio = 0.2\n",
        "  max_frame = 100\n",
        "  data_path = \"/content/asl-signs/\"\n",
        "  smoothing = 0.75\n",
        "  fold_n = 5\n",
        "\n",
        "  # modeling\n",
        "  in_features = rows_per_frame * 3\n",
        "  out_features = 32\n",
        "  hidden_size = 64\n",
        "  dense_dim = 512\n",
        "  num_classes = 250\n",
        "  drop_rate = 0.4\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  args = CustomConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8VdKhkkFhYa"
      },
      "source": [
        "## seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhnl1-y4F0qI"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 1):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    keras.utils.set_random_seed(seed)\n",
        "\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl6di3bCFis4"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulqEvvkDF2tx"
      },
      "outputs": [],
      "source": [
        "data = np.load('/content/drive/MyDrive/Kaggle/aggregation/data_m_with_lip.npy')\n",
        "label = np.load('/content/drive/MyDrive/Kaggle/aggregation/label.npy')\n",
        "frame = np.load('/content/drive/MyDrive/Kaggle/aggregation/frame.npy')\n",
        "batch_id = np.load('/content/drive/MyDrive/Kaggle/aggregation/batch_id.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBU3HhGFkB3"
      },
      "source": [
        "## preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD_406qyGDqN"
      },
      "outputs": [],
      "source": [
        "def preprocess(args):\n",
        "  participant_ids = np.array([26734, 28656, 16069, 25571, 62590, 32319, 37055, 29302, 49445,\n",
        "                              36257, 22343, 27610, 61333, 53618, 34503, 18796,  4718, 55372,\n",
        "                              2044, 37779, 30680])\n",
        "                            \n",
        "  df = pd.read_csv('/content/drive/MyDrive/Kaggle/train.csv')#df = pd.DataFrame()\n",
        "  df['frame'] = frame\n",
        "  df['label'] = label\n",
        "  df['original_index'] = np.arange(len(df))\n",
        "  \n",
        "  kf = KFold(n_splits = args.fold_n, shuffle = False)\n",
        "\n",
        "  folds = list()\n",
        "  for train_index, test_index in kf.split(participant_ids):\n",
        "    train_ids = participant_ids[train_index]\n",
        "    test_ids = participant_ids[test_index]\n",
        "\n",
        "    train_df = df[df['participant_id'].isin(train_ids)].reset_index(drop = True)\n",
        "    test_df = df[df['participant_id'].isin(test_ids)].reset_index(drop = True)\n",
        "\n",
        "    col = ['frame', 'label', 'original_index']\n",
        "    folds.append([train_df[col], test_df[col]])\n",
        "  return folds, df[col]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  folds, df = preprocess(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XW6GYiFlw7"
      },
      "source": [
        "## feature gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCIn5sQTHY26"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureGenPytorch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureGenPytorch, self).__init__()\n",
        "        self.htriu = torch.tensor([[0] * (bi + 1) + [1] * (20 - bi) for bi in range(21)], dtype = torch.float).unsqueeze(0)\n",
        "        self.ptriu = torch.tensor([[0] * (bi + 1) + [1] * (24 - bi) for bi in range(25)], dtype = torch.float).unsqueeze(0)\n",
        "        self.ltriu = torch.tensor([[0] * (bi + 1) + [1] * (19 - bi) for bi in range(20)], dtype = torch.float).unsqueeze(0)\n",
        "        self.lip_indices = [\n",
        "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
        "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308\n",
        "            ]\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.where(torch.isnan(x), torch.tensor(0.0, dtype=torch.float32), x)\n",
        "\n",
        "        #lefth_x = x[:, 468:489, :]\n",
        "        #righth_x = x[:, 522:, :]\n",
        "        #pose_x = x[:, 489:514, :]\n",
        "        #lip_x = x[:, self.lip_indices, :]\n",
        "\n",
        "        lefth_x = x[:,40:61,:]\n",
        "        righth_x = x[:,94:,:]\n",
        "        pose_x = x[:, 61:86, :]#[:, self.simple_pose]\n",
        "        lip_x = x[:, :40, :]\n",
        "        \n",
        "        lefth_sum = (lefth_x!=0).float().sum()\n",
        "        righth_sum = (righth_x!=0).float().sum()\n",
        "        \n",
        "        cond = lefth_sum > righth_sum\n",
        "            \n",
        "        h_x = torch.where(cond, lefth_x, righth_x)\n",
        "        xfeat = torch.where(cond, torch.cat([lefth_x, pose_x, lip_x], dim = 1), torch.cat([righth_x, pose_x, lip_x], dim = 1) )\n",
        "\n",
        "        xfeat_xcoordi = xfeat[:, :, 0]\n",
        "        xfeat_else = xfeat[:, :, 1:]\n",
        "        xfeat_xcoordi = torch.where(cond, -xfeat_xcoordi, xfeat_xcoordi)\n",
        "        xfeat = torch.cat([xfeat_xcoordi.unsqueeze(2), xfeat_else], dim = -1)\n",
        "        \n",
        "        h_x = h_x.reshape(h_x.shape[0], -1) \n",
        "        indices = (h_x.sum(1) != 0)\n",
        "        if indices.sum() != 0:\n",
        "            xfeat = xfeat[indices]\n",
        "\n",
        "        dxyz = torch.cat([xfeat[:-1] - xfeat[1:], torch.zeros(1, xfeat.shape[1], xfeat.shape[2])], dim = 0)\n",
        "        \n",
        "        hand = xfeat[:, :21, :3]\n",
        "        hd = hand.reshape(-1, 21, 1, 3) - hand.reshape(-1, 1, 21, 3)\n",
        "        hd = torch.sqrt((hd ** 2).sum(-1)) + 1\n",
        "        hd = hd * self.htriu\n",
        "        indices = (hd.reshape(hd.shape[0], -1)!=0)\n",
        "        hd = hd.reshape(hd.shape[0], -1)[indices].reshape(hd.shape[0], -1)\n",
        "        hdist = hd - 1\n",
        "        \n",
        "        pose = xfeat[:, 21:46, :2]\n",
        "        pd = pose.reshape(-1, 25, 1, 2) - pose.reshape(-1, 1, 25, 2)\n",
        "        pd = torch.sqrt((pd ** 2).sum(-1)) + 1\n",
        "        pd = pd * self.ptriu\n",
        "        indices = (pd.reshape(pd.shape[0], -1)!=0)\n",
        "        pd = pd.reshape(pd.shape[0], -1)[indices].reshape(pd.shape[0], -1)\n",
        "        pdist = pd - 1\n",
        "\n",
        "        olip = xfeat[:, 46:66, :2]\n",
        "        old = olip.reshape(-1, 20, 1, 2) - olip.reshape(-1, 1, 20, 2)\n",
        "        old = torch.sqrt((old ** 2).sum(-1)) + 1\n",
        "        old = old * self.ltriu\n",
        "        indices = (old.reshape(old.shape[0], -1)!=0)\n",
        "        old = old.reshape(old.shape[0], -1)[indices].reshape(old.shape[0], -1)\n",
        "        oldist = old\n",
        "        oldist = oldist - 1\n",
        "\n",
        "        ilip = xfeat[:, 66:86, :2]\n",
        "        ild = ilip.reshape(-1, 20, 1, 2) - ilip.reshape(-1, 1, 20, 2)\n",
        "        ild = torch.sqrt((ild ** 2).sum(-1)) + 1\n",
        "        ild = ild * self.ltriu\n",
        "        indices = (ild.reshape(ild.shape[0], -1)!=0)\n",
        "        ild = ild.reshape(ild.shape[0], -1)[indices].reshape(ild.shape[0], -1)\n",
        "        ildist = ild\n",
        "        ildist = ildist - 1\n",
        "        \n",
        "        \n",
        "        xfeat = torch.cat([\n",
        "            xfeat[:, :21, :3].reshape(xfeat.shape[0], -1), \n",
        "            xfeat[:, 21:46, :2].reshape(xfeat.shape[0], -1), \n",
        "            xfeat[:, 46:66, :2].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, :21, :3].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, 21:46, :2].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, 46:66, :2].reshape(xfeat.shape[0], -1), \n",
        "            hdist.reshape(xfeat.shape[0], -1),\n",
        "            pdist.reshape(xfeat.shape[0], -1),\n",
        "            oldist.reshape(xfeat.shape[0], -1),\n",
        "            ildist.reshape(xfeat.shape[0], -1),\n",
        "        ], dim = -1)\n",
        "        \n",
        "        xfeat = xfeat[:100]\n",
        "        #pad_length = 100 - xfeat.shape[0]\n",
        "        #xfeat = torch.cat([xfeat, torch.zeros(pad_length, xfeat.shape[1])])\n",
        "        #xfeat = xfeat.reshape(100, 1196)\n",
        "        \n",
        "        return xfeat\n",
        "\n",
        "feature_converter_pt = FeatureGenPytorch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcMzne5non-q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureGenPytorchV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureGenPytorchV2, self).__init__()\n",
        "        self.htriu = torch.tensor([[0] * (bi + 1) + [1] * (20 - bi) for bi in range(21)], dtype = torch.float).unsqueeze(0)\n",
        "        self.ptriu = torch.tensor([[0] * (bi + 1) + [1] * (24 - bi) for bi in range(25)], dtype = torch.float).unsqueeze(0)\n",
        "        self.ltriu = torch.tensor([[0] * (bi + 1) + [1] * (19 - bi) for bi in range(20)], dtype = torch.float).unsqueeze(0)\n",
        "        self.lip_indices = [\n",
        "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
        "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308\n",
        "            ]\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x[:200]\n",
        "        x = torch.where(torch.isnan(x), torch.tensor(0.0, dtype=torch.float32), x)\n",
        "\n",
        "        #lefth_x = x[:, 468:489, :]\n",
        "        #righth_x = x[:, 522:, :]\n",
        "        #pose_x = x[:, 489:514, :]\n",
        "        #lip_x = x[:, self.lip_indices, :]\n",
        "\n",
        "        lefth_x = x[:,40:61,:]\n",
        "        righth_x = x[:,94:,:]\n",
        "        pose_x = x[:, 61:86, :]#[:, self.simple_pose]\n",
        "        lip_x = x[:, :40, :]\n",
        "        \n",
        "        lefth_sum = (lefth_x!=0).float().sum()\n",
        "        righth_sum = (righth_x!=0).float().sum()\n",
        "        \n",
        "        cond = lefth_sum > righth_sum\n",
        "            \n",
        "        h_x = torch.where(cond, lefth_x, righth_x)\n",
        "        xfeat = torch.where(cond, torch.cat([lefth_x, pose_x, lip_x], dim = 1), torch.cat([righth_x, pose_x, lip_x], dim = 1) )\n",
        "\n",
        "        xfeat_xcoordi = xfeat[:, :, 0]\n",
        "        xfeat_else = xfeat[:, :, 1:]\n",
        "        xfeat_xcoordi = torch.where(cond, -xfeat_xcoordi, xfeat_xcoordi)\n",
        "        xfeat = torch.cat([xfeat_xcoordi.unsqueeze(2), xfeat_else], dim = -1)\n",
        "        \n",
        "        h_x = h_x.reshape(h_x.shape[0], -1) \n",
        "        #indices = (h_x.sum(1) != 0)\n",
        "        #if indices.sum() != 0:\n",
        "        #    xfeat = xfeat[indices]\n",
        "        hand_mask = (h_x.sum(1) != 0)\n",
        "        if hand_mask.sum()==0:\n",
        "          print(0)\n",
        "        token_type_ids = (h_x.sum(1) != 0) + 1\n",
        "\n",
        "        dxyz = torch.cat([xfeat[:-1] - xfeat[1:], torch.zeros(1, xfeat.shape[1], xfeat.shape[2])], dim = 0)\n",
        "        \n",
        "        hand = xfeat[:, :21, :3]\n",
        "        hd = hand.reshape(-1, 21, 1, 3) - hand.reshape(-1, 1, 21, 3)\n",
        "        hd = torch.sqrt((hd ** 2).sum(-1)) + 1\n",
        "        hd = hd * self.htriu\n",
        "        indices = (hd.reshape(hd.shape[0], -1)!=0)\n",
        "        hd = hd.reshape(hd.shape[0], -1)[indices].reshape(hd.shape[0], -1)\n",
        "        hdist = hd - 1\n",
        "        \n",
        "        pose = xfeat[:, 21:46, :2]\n",
        "        pd = pose.reshape(-1, 25, 1, 2) - pose.reshape(-1, 1, 25, 2)\n",
        "        pd = torch.sqrt((pd ** 2).sum(-1)) + 1\n",
        "        pd = pd * self.ptriu\n",
        "        indices = (pd.reshape(pd.shape[0], -1)!=0)\n",
        "        pd = pd.reshape(pd.shape[0], -1)[indices].reshape(pd.shape[0], -1)\n",
        "        pdist = pd - 1\n",
        "\n",
        "        olip = xfeat[:, 46:66, :2]\n",
        "        old = olip.reshape(-1, 20, 1, 2) - olip.reshape(-1, 1, 20, 2)\n",
        "        old = torch.sqrt((old ** 2).sum(-1)) + 1\n",
        "        old = old * self.ltriu\n",
        "        indices = (old.reshape(old.shape[0], -1)!=0)\n",
        "        old = old.reshape(old.shape[0], -1)[indices].reshape(old.shape[0], -1)\n",
        "        oldist = old\n",
        "        oldist = oldist - 1\n",
        "\n",
        "        ilip = xfeat[:, 66:86, :2]\n",
        "        ild = ilip.reshape(-1, 20, 1, 2) - ilip.reshape(-1, 1, 20, 2)\n",
        "        ild = torch.sqrt((ild ** 2).sum(-1)) + 1\n",
        "        ild = ild * self.ltriu\n",
        "        indices = (ild.reshape(ild.shape[0], -1)!=0)\n",
        "        ild = ild.reshape(ild.shape[0], -1)[indices].reshape(ild.shape[0], -1)\n",
        "        ildist = ild\n",
        "        ildist = ildist - 1\n",
        "        \n",
        "        \n",
        "        xfeat = torch.cat([\n",
        "            xfeat[:, :21, :3].reshape(xfeat.shape[0], -1), \n",
        "            xfeat[:, 21:46, :2].reshape(xfeat.shape[0], -1), \n",
        "            xfeat[:, 46:66, :2].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, :21, :3].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, 21:46, :2].reshape(xfeat.shape[0], -1), \n",
        "            dxyz[:, 46:66, :2].reshape(xfeat.shape[0], -1), \n",
        "            hdist.reshape(xfeat.shape[0], -1),\n",
        "            pdist.reshape(xfeat.shape[0], -1),\n",
        "            oldist.reshape(xfeat.shape[0], -1),\n",
        "            ildist.reshape(xfeat.shape[0], -1),\n",
        "            hand_mask.reshape(xfeat.shape[0], -1),\n",
        "            token_type_ids.reshape(xfeat.shape[0], -1)\n",
        "        ], dim = -1)\n",
        "        \n",
        "        xfeat = xfeat[:200]\n",
        "        #pad_length = 100 - xfeat.shape[0]\n",
        "        #xfeat = torch.cat([xfeat, torch.zeros(pad_length, xfeat.shape[1])])\n",
        "        #xfeat = xfeat.reshape(100, 1196)\n",
        "        \n",
        "        return xfeat\n",
        "\n",
        "feature_converter_pt_v2 = FeatureGenPytorchV2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl_5bgF5GFu_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class FeatureGenKeras(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(FeatureGenKeras, self).__init__()\n",
        "        self.htriu = tf.constant([[0] * (bi + 1) + [1] * (20 - bi) for bi in range(21)], dtype = tf.float32)\n",
        "        self.ptriu = tf.constant([[0] * (bi + 1) + [1] * (24 - bi) for bi in range(25)], dtype = tf.float32)\n",
        "        self.ltriu = tf.constant([[0] * (bi + 1) + [1] * (19 - bi) for bi in range(20)], dtype = tf.float32)\n",
        "        self.lip_indices = tf.constant([\n",
        "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
        "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308\n",
        "            ])\n",
        "\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, dtype=tf.float32), x)\n",
        "        xfeat = x[:, 468:, :]\n",
        "\n",
        "        #lefth_x = x[:, 468:489, :]\n",
        "        #righth_x = x[:, 522:, :]\n",
        "        #pose_x = x[:, 489:514, :]\n",
        "        #lip_x = tf.gather(x, self.lip_indices, axis=1)#x[:, self.lip_indices, :]\n",
        "\n",
        "        lefth_x = x[:,40:61,:]\n",
        "        righth_x = x[:,94:,:]\n",
        "        pose_x = x[:, 61:86, :]#[:, self.simple_pose]\n",
        "        lip_x = x[:, :40, :]\n",
        "        \n",
        "        lefth_sum = tf.reduce_sum(tf.cast(tf.not_equal(lefth_x, 0), dtype=tf.float32))\n",
        "        righth_sum = tf.reduce_sum(tf.cast(tf.not_equal(righth_x, 0), dtype=tf.float32))\n",
        "        \n",
        "        cond = lefth_sum > righth_sum\n",
        "            \n",
        "        h_x = tf.where(cond, lefth_x, righth_x)\n",
        "        xfeat = tf.where(cond, tf.concat([lefth_x, pose_x, lip_x], axis = 1), tf.concat([righth_x, pose_x, lip_x], axis = 1))\n",
        "        \n",
        "        xfeat_xcoordi = xfeat[:, :, 0]\n",
        "        xfeat_else = xfeat[:, :, 1:]\n",
        "        xfeat_xcoordi = tf.where(cond, -xfeat_xcoordi, xfeat_xcoordi)\n",
        "        xfeat = tf.concat([xfeat_xcoordi[:, :, tf.newaxis], xfeat_else], axis = -1)\n",
        "        \n",
        "        h_x = tf.reshape(h_x, (-1, 21 * 3))\n",
        "        indices = tf.squeeze(tf.math.reduce_sum(h_x, axis=1) != 0)\n",
        "\n",
        "        dynamic_size = tf.shape(h_x)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size,))\n",
        "\n",
        "        xfeat = tf.boolean_mask(xfeat, indices)\n",
        "\n",
        "        dxyz = tf.concat([xfeat[:-1] - xfeat[1:], tf.zeros((1, xfeat.shape[1], xfeat.shape[2]))], axis = 0)\n",
        "        \n",
        "        # hand\n",
        "        hand = xfeat[:, :21, :3]\n",
        "        hdist = tf.reshape(hand, (-1, 21, 1, 3)) - tf.reshape(hand, (-1, 1, 21, 3))\n",
        "        hdist = tf.sqrt(tf.reduce_sum(tf.square(hdist), axis=-1)) + 1\n",
        "        hdist = hdist * self.htriu\n",
        "        indices = tf.reshape(hdist, (-1, 21 * 21)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(hdist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 21 * 21))\n",
        "        hdist = tf.boolean_mask(tf.reshape(hdist, (-1, 21 * 21)), indices)\n",
        "        hdist = hdist - 1\n",
        "        \n",
        "        # pose\n",
        "        pose = xfeat[:, 21:46, :2]\n",
        "        pdist = tf.reshape(pose, (-1, 25, 1, 2)) - tf.reshape(pose, (-1, 1, 25, 2))\n",
        "        pdist = tf.sqrt(tf.reduce_sum(tf.square(pdist), axis=-1)) + 1\n",
        "        pdist = pdist * self.ptriu\n",
        "        indices = tf.reshape(pdist, (-1, 25 * 25)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(pdist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 25 * 25))\n",
        "        pdist = tf.boolean_mask(tf.reshape(pdist, (-1, 25 * 25)), indices)\n",
        "        pdist = pdist - 1\n",
        "        \n",
        "        # outlip\n",
        "        olip = xfeat[:, 46:66, :2]\n",
        "        oldist = tf.reshape(olip, (-1, 20, 1, 2)) - tf.reshape(olip, (-1, 1, 20, 2))\n",
        "        oldist = tf.sqrt(tf.reduce_sum(tf.square(oldist), axis=-1)) + 1\n",
        "        oldist = oldist * self.ltriu\n",
        "        indices = tf.reshape(oldist, (-1, 20 * 20)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(oldist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 20 * 20))\n",
        "        oldist = tf.boolean_mask(tf.reshape(oldist, (-1, 20 * 20)), indices)\n",
        "        oldist = oldist - 1\n",
        "        \n",
        "        # inlip\n",
        "        ilip = xfeat[:, 66:86, :2]\n",
        "        ildist = tf.reshape(ilip, (-1, 20, 1, 2)) - tf.reshape(ilip, (-1, 1, 20, 2))\n",
        "        ildist = tf.sqrt(tf.reduce_sum(tf.square(ildist), axis=-1)) + 1\n",
        "        ildist = ildist * self.ltriu\n",
        "        indices = tf.reshape(ildist, (-1, 20 * 20)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(ildist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 20 * 20))\n",
        "        ildist = tf.boolean_mask(tf.reshape(ildist, (-1, 20 * 20)), indices)\n",
        "        ildist = ildist - 1\n",
        "        \n",
        "        xfeat = tf.concat([\n",
        "            tf.reshape(xfeat[:, :21, :3], [-1, 21 * 3]), \n",
        "            tf.reshape(xfeat[:, 21:46, :2], [-1, 25 * 2]), \n",
        "            tf.reshape(xfeat[:, 46:66, :2], [-1, 20 * 2]), \n",
        "            tf.reshape(dxyz[:, :21, :3], [-1, 21 * 3]), \n",
        "            tf.reshape(dxyz[:, 21:46, :2], [-1, 25 * 2]), \n",
        "            tf.reshape(dxyz[:, 46:66, :2], [-1, 20 * 2]), \n",
        "            tf.reshape(hdist, [-1, 210]),\n",
        "            tf.reshape(pdist, [-1, 300]),\n",
        "            tf.reshape(oldist, [-1, 190]),\n",
        "            tf.reshape(ildist, [-1, 190]),\n",
        "        ], axis=-1)\n",
        "        \n",
        "        xfeat = xfeat[:100]\n",
        "        #pad_length = 100 - xfeat.shape[0]\n",
        "        #xfeat = tf.concat([xfeat, tf.zeros((pad_length, xfeat.shape[1]), dtype=tf.float32)], axis = 0)\n",
        "        xfeat = tf.reshape(xfeat, (1, -1, 1196))\n",
        "        \n",
        "        return xfeat\n",
        "\n",
        "feature_converter_kr = FeatureGenKeras()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839H4vFpoo8-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class FeatureGenKerasV2(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(FeatureGenKerasV2, self).__init__()\n",
        "        self.htriu = tf.constant([[0] * (bi + 1) + [1] * (20 - bi) for bi in range(21)], dtype = tf.float32)\n",
        "        self.ptriu = tf.constant([[0] * (bi + 1) + [1] * (24 - bi) for bi in range(25)], dtype = tf.float32)\n",
        "        self.ltriu = tf.constant([[0] * (bi + 1) + [1] * (19 - bi) for bi in range(20)], dtype = tf.float32)\n",
        "        self.lip_indices = tf.constant([\n",
        "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
        "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308\n",
        "            ])\n",
        "\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = x[:200]\n",
        "        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, dtype=tf.float32), x)\n",
        "        xfeat = x[:, 468:, :]\n",
        "\n",
        "        #lefth_x = x[:, 468:489, :]\n",
        "        #righth_x = x[:, 522:, :]\n",
        "        #pose_x = x[:, 489:514, :]\n",
        "        #lip_x = tf.gather(x, self.lip_indices, axis=1)#x[:, self.lip_indices, :]\n",
        "\n",
        "        lefth_x = x[:,40:61,:]\n",
        "        righth_x = x[:,94:,:]\n",
        "        pose_x = x[:, 61:86, :]#[:, self.simple_pose]\n",
        "        lip_x = x[:, :40, :]\n",
        "        \n",
        "        lefth_sum = tf.reduce_sum(tf.cast(tf.not_equal(lefth_x, 0), dtype=tf.float32))\n",
        "        righth_sum = tf.reduce_sum(tf.cast(tf.not_equal(righth_x, 0), dtype=tf.float32))\n",
        "        \n",
        "        cond = lefth_sum > righth_sum\n",
        "            \n",
        "        h_x = tf.where(cond, lefth_x, righth_x)\n",
        "        xfeat = tf.where(cond, tf.concat([lefth_x, pose_x, lip_x], axis = 1), tf.concat([righth_x, pose_x, lip_x], axis = 1))\n",
        "        \n",
        "        xfeat_xcoordi = xfeat[:, :, 0]\n",
        "        xfeat_else = xfeat[:, :, 1:]\n",
        "        xfeat_xcoordi = tf.where(cond, -xfeat_xcoordi, xfeat_xcoordi)\n",
        "        xfeat = tf.concat([xfeat_xcoordi[:, :, tf.newaxis], xfeat_else], axis = -1)\n",
        "        \n",
        "        h_x = tf.reshape(h_x, (-1, 21 * 3))\n",
        "        indices = tf.squeeze(tf.math.reduce_sum(h_x, axis=1) != 0)\n",
        "\n",
        "        dynamic_size = tf.shape(h_x)[0]\n",
        "        #indices = tf.reshape(indices, (dynamic_size,))\n",
        "\n",
        "        #xfeat = tf.boolean_mask(xfeat, indices)\n",
        "        indices = tf.reshape(indices, (dynamic_size,))\n",
        "        indices = tf.cast(indices, dtype = tf.float32)\n",
        "        hand_mask = indices + 0.0\n",
        "        token_type_ids = indices + 1.0\n",
        "\n",
        "        dxyz = tf.concat([xfeat[:-1] - xfeat[1:], tf.zeros((1, xfeat.shape[1], xfeat.shape[2]))], axis = 0)\n",
        "        \n",
        "        # hand\n",
        "        hand = xfeat[:, :21, :3]\n",
        "        hdist = tf.reshape(hand, (-1, 21, 1, 3)) - tf.reshape(hand, (-1, 1, 21, 3))\n",
        "        hdist = tf.sqrt(tf.reduce_sum(tf.square(hdist), axis=-1)) + 1\n",
        "        hdist = hdist * self.htriu\n",
        "        indices = tf.reshape(hdist, (-1, 21 * 21)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(hdist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 21 * 21))\n",
        "        hdist = tf.boolean_mask(tf.reshape(hdist, (-1, 21 * 21)), indices)\n",
        "        hdist = hdist - 1\n",
        "        \n",
        "        # pose\n",
        "        pose = xfeat[:, 21:46, :2]\n",
        "        pdist = tf.reshape(pose, (-1, 25, 1, 2)) - tf.reshape(pose, (-1, 1, 25, 2))\n",
        "        pdist = tf.sqrt(tf.reduce_sum(tf.square(pdist), axis=-1)) + 1\n",
        "        pdist = pdist * self.ptriu\n",
        "        indices = tf.reshape(pdist, (-1, 25 * 25)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(pdist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 25 * 25))\n",
        "        pdist = tf.boolean_mask(tf.reshape(pdist, (-1, 25 * 25)), indices)\n",
        "        pdist = pdist - 1\n",
        "        \n",
        "        # outlip\n",
        "        olip = xfeat[:, 46:66, :2]\n",
        "        oldist = tf.reshape(olip, (-1, 20, 1, 2)) - tf.reshape(olip, (-1, 1, 20, 2))\n",
        "        oldist = tf.sqrt(tf.reduce_sum(tf.square(oldist), axis=-1)) + 1\n",
        "        oldist = oldist * self.ltriu\n",
        "        indices = tf.reshape(oldist, (-1, 20 * 20)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(oldist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 20 * 20))\n",
        "        oldist = tf.boolean_mask(tf.reshape(oldist, (-1, 20 * 20)), indices)\n",
        "        oldist = oldist - 1\n",
        "        \n",
        "        # inlip\n",
        "        ilip = xfeat[:, 66:86, :2]\n",
        "        ildist = tf.reshape(ilip, (-1, 20, 1, 2)) - tf.reshape(ilip, (-1, 1, 20, 2))\n",
        "        ildist = tf.sqrt(tf.reduce_sum(tf.square(ildist), axis=-1)) + 1\n",
        "        ildist = ildist * self.ltriu\n",
        "        indices = tf.reshape(ildist, (-1, 20 * 20)) != 0\n",
        "        \n",
        "        dynamic_size = tf.shape(ildist)[0]\n",
        "        indices = tf.reshape(indices, (dynamic_size, 20 * 20))\n",
        "        ildist = tf.boolean_mask(tf.reshape(ildist, (-1, 20 * 20)), indices)\n",
        "        ildist = ildist - 1\n",
        "        \n",
        "        xfeat = tf.concat([\n",
        "            tf.reshape(xfeat[:, :21, :3], [-1, 21 * 3]), \n",
        "            tf.reshape(xfeat[:, 21:46, :2], [-1, 25 * 2]), \n",
        "            tf.reshape(xfeat[:, 46:66, :2], [-1, 20 * 2]), \n",
        "            tf.reshape(dxyz[:, :21, :3], [-1, 21 * 3]), \n",
        "            tf.reshape(dxyz[:, 21:46, :2], [-1, 25 * 2]), \n",
        "            tf.reshape(dxyz[:, 46:66, :2], [-1, 20 * 2]), \n",
        "            tf.reshape(hdist, [-1, 210]),\n",
        "            tf.reshape(pdist, [-1, 300]),\n",
        "            tf.reshape(oldist, [-1, 190]),\n",
        "            tf.reshape(ildist, [-1, 190]),\n",
        "            tf.reshape(hand_mask, [-1, 1]),\n",
        "            tf.reshape(token_type_ids, [-1, 1])\n",
        "        ], axis=-1)\n",
        "        \n",
        "        xfeat = xfeat[:200]\n",
        "        #pad_length = 100 - xfeat.shape[0]\n",
        "        #xfeat = tf.concat([xfeat, tf.zeros((pad_length, xfeat.shape[1]), dtype=tf.float32)], axis = 0)\n",
        "        xfeat = tf.reshape(xfeat, (1, -1, 1198))\n",
        "        \n",
        "        return xfeat\n",
        "\n",
        "feature_converter_kr_v2 = FeatureGenKerasV2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqC7nYx-GHRv",
        "outputId": "2808717b-fdf6-45c1-c0d9-20f25d1f2b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20748.96\n",
            "0.7069005966186523\n",
            "20748.96\n",
            "0.06843876838684082\n",
            "20784.959\n",
            "0.03371691703796387\n",
            "20784.959\n",
            "0.003103971481323242\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(12, 115, 3).numpy()\n",
        "\n",
        "time_kr = time.time()\n",
        "print(feature_converter_kr(x).numpy().sum())\n",
        "print(time.time() - time_kr)\n",
        "\n",
        "time_pt = time.time()\n",
        "print(feature_converter_pt(torch.Tensor(x)).numpy().sum())\n",
        "print(time.time() - time_pt)\n",
        "\n",
        "time_kr = time.time()\n",
        "print(feature_converter_kr_v2(x).numpy().sum())\n",
        "print(time.time() - time_kr)\n",
        "\n",
        "time_pt = time.time()\n",
        "print(feature_converter_pt_v2(torch.Tensor(x)).numpy().sum())\n",
        "print(time.time() - time_pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bdAI5YxFncb"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzmxjcLJGItM"
      },
      "outputs": [],
      "source": [
        "def cumulative_sum_tuples(lst):\n",
        "  result = [(0, lst[0])]\n",
        "  if len(lst) > 0:\n",
        "    cum_sum = lst[0]\n",
        "    for i in range(1, len(lst)):\n",
        "      cum_sum += lst[i]\n",
        "      result.append((cum_sum-lst[i], cum_sum))\n",
        "  return result\n",
        "\n",
        "def pad(self, x, max_frame):\n",
        "  if x.shape[0] > max_frame:\n",
        "    padded_x = x[:max_frame]\n",
        "  else:\n",
        "    padded_x = torch.zeros(max_frame, x.shape[-1])\n",
        "    padded_x[:x.shape[0]] = x\n",
        "\n",
        "  return padded_x\n",
        "\n",
        "frame = np.load('/content/drive/MyDrive/Kaggle/aggregation/frame.npy')\n",
        "frame_index = cumulative_sum_tuples(frame.astype(int))\n",
        "def get_data(df, version):\n",
        "  frame, label, original_index = df\n",
        "\n",
        "  start_idx, end_idx = frame_index[int(original_index)]\n",
        "  x = torch.tensor(data[start_idx:end_idx], dtype = torch.float)\n",
        "  if version in ['v1', 'v4', 'v5']:\n",
        "    x = feature_converter_pt(x)\n",
        "  elif version in ['v2', 'v3']:\n",
        "    x = feature_converter_pt_v2(x)\n",
        "  y = tf.one_hot(int(label), 250)\n",
        "\n",
        "  x = x.numpy()\n",
        "  y = y.numpy()\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def get_inputs(df, version):\n",
        "  if version in ['v1', 'v4', 'v5']:\n",
        "    inputs_x = np.zeros((len(df), 100, 1196), dtype=np.float32)\n",
        "  elif version in ['v2', 'v3']:\n",
        "    inputs_x = np.zeros((len(df), 200, 1198), dtype=np.float32)\n",
        "  inputs_y = np.zeros((len(df), 250), dtype=np.float32)\n",
        "\n",
        "  for i in tqdm(range(len(df))):\n",
        "    x, y = get_data(df.iloc[i], version)\n",
        "    inputs_x[i, :x.shape[0]] = x\n",
        "    inputs_y[i, :] = y\n",
        "  return inputs_x, inputs_y\n",
        "\n",
        "class Dataloader(Sequence):\n",
        "  def __init__(self, args, df, data, batch_size, version, shuffle=False):\n",
        "    self.args = args\n",
        "    self.df = df\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.version = version\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "    self.on_epoch_end()\n",
        "    \n",
        "    self.x, self.y = get_inputs(df, version)\n",
        "\n",
        "    self.indices = np.arange(len(self.df))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "    batch_x = [self.x[i] for i in indices]\n",
        "    batch_y = [self.y[i] for i in indices]\n",
        "\n",
        "    return tf.convert_to_tensor(np.array(batch_x)), tf.convert_to_tensor(np.array(batch_y))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indices = np.arange(len(self.df))\n",
        "    if self.shuffle == True:\n",
        "      np.random.shuffle(self.indices)\n",
        "\n",
        "  def __len__(self):\n",
        "      return math.ceil(len(self.df) / self.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXbHRIskFoeK"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "521efTGpQPyr",
        "outputId": "cb16b6de-4b42-461c-cdc0-4135e81aaaf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 250) dtype=float32 (created by layer 'custom_model')>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from transformers import TFRobertaPreLayerNormModel, TFDebertaV2Model, TFGPT2Model, RobertaPreLayerNormConfig, DebertaV2Config, GPT2Config\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModel, self).__init__()\n",
        "        \n",
        "        self.args = args\n",
        "        self.hidden = 300\n",
        "\n",
        "        self.xy_embeddings = keras.layers.Dense(units=self.hidden, name=\"xy_embeddings\")\n",
        "        self.motion_embeddings = keras.layers.Dense(units=self.hidden, name=\"motion_embeddings\")\n",
        "        self.hdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"hdist_embeddings\")\n",
        "        self.pdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"pdist_embeddings\")\n",
        "        self.oldist_embeddings = keras.layers.Dense(units=self.hidden, name=\"oldist_embeddings\")\n",
        "        self.ildist_embeddings = keras.layers.Dense(units=self.hidden, name=\"ildist_embeddings\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.content_embeddings = keras.layers.Dense(units=self.hidden, name=\"content_embeddings\")\n",
        "        \n",
        "        if args == 'tfrobertaprelayernorm':\n",
        "          self.encoder = TFRobertaPreLayerNormModel(\n",
        "              RobertaPreLayerNormConfig(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        elif args == 'tfdebertav2':\n",
        "          self.encoder = TFDebertaV2Model(\n",
        "              DebertaV2Config(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(units=1024, name=\"fc\")\n",
        "        self.bn = keras.layers.BatchNormalization(name=\"bn\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.drop = keras.layers.Dropout(rate=0.4, name=\"drop\")\n",
        "\n",
        "        self.out = keras.layers.Dense(units=250, activation='softmax', name=\"out\")\n",
        "\n",
        "        self.xy_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.motion_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.hdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.pdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.oldist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.ildist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.content_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.fc.kernel_initializer = 'glorot_uniform'\n",
        "        self.out.kernel_initializer = 'glorot_uniform'\n",
        "\n",
        "    def get_att_mask(self, x):\n",
        "        att_mask = tf.math.reduce_sum(x, axis=-1)\n",
        "        att_mask = tf.cast(tf.math.not_equal(att_mask, 0), tf.float32)\n",
        "        return att_mask\n",
        "\n",
        "    def get_pool(self, x, x_mask):\n",
        "        x = x * tf.expand_dims(x_mask, axis=-1)  # apply mask\n",
        "        nonzero_count = tf.reduce_sum(x_mask, axis=1, keepdims=True)  # count nonzero elements\n",
        "        max_discount = (1-x_mask)*1e10\n",
        "\n",
        "        apool = tf.reduce_sum(x, axis=1) / nonzero_count\n",
        "        mpool = tf.reduce_max(x - tf.expand_dims(max_discount, axis=-1), axis=1)\n",
        "        spool = tf.sqrt((tf.reduce_sum(((x - tf.expand_dims(apool, axis=1)) ** 2) * tf.expand_dims(x_mask, axis=-1), axis=1) / nonzero_count) + 1e-9)\n",
        "        return tf.concat([apool, mpool, spool], axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x_mask = self.get_att_mask(x)\n",
        "\n",
        "        xy = self.xy_embeddings(x[:, :, :153])\n",
        "        motion = self.motion_embeddings(x[:, :, 153:306])\n",
        "        dist = self.hdist_embeddings(x[:, :, 306:516])\n",
        "        pdist = self.pdist_embeddings(x[:, :, 516:816])\n",
        "        oldist = self.oldist_embeddings(x[:, :, 816:1006])\n",
        "        ildist = self.ildist_embeddings(x[:, :, 1006:1196])\n",
        "\n",
        "        x = tf.concat([xy, motion, dist, pdist, oldist, ildist], axis=-1)\n",
        "        x = self.relu(x)\n",
        "        x = self.content_embeddings(x)\n",
        "        x = self.encoder(input_ids = None, inputs_embeds=x, attention_mask=x_mask).last_hidden_state\n",
        "\n",
        "        x = self.get_pool(x, x_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "model = CustomModel('tfrobertaprelayernorm')\n",
        "\n",
        "input_shape = (None, 1196)  # dynamic input shape\n",
        "\n",
        "# Create a model with an InputLayer to allow dynamic input shape\n",
        "inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po2IQbVDqp0L",
        "outputId": "55efc068-75ba-45ca-fcab-9ffb13d193d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 250) dtype=float32 (created by layer 'custom_model_v2')>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from transformers import TFRobertaPreLayerNormModel, TFDebertaV2Model, RobertaPreLayerNormConfig, DebertaV2Config\n",
        "\n",
        "class CustomModelV2(keras.Model):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV2, self).__init__()\n",
        "        \n",
        "        self.args = args\n",
        "        self.hidden = 300\n",
        "\n",
        "        self.xy_embeddings = keras.layers.Dense(units=self.hidden, name=\"xy_embeddings\")\n",
        "        self.motion_embeddings = keras.layers.Dense(units=self.hidden, name=\"motion_embeddings\")\n",
        "        self.hdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"hdist_embeddings\")\n",
        "        self.pdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"pdist_embeddings\")\n",
        "        self.oldist_embeddings = keras.layers.Dense(units=self.hidden, name=\"oldist_embeddings\")\n",
        "        self.ildist_embeddings = keras.layers.Dense(units=self.hidden, name=\"ildist_embeddings\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.content_embeddings = keras.layers.Dense(units=self.hidden, name=\"content_embeddings\")\n",
        "        \n",
        "        if args == 'tfrobertaprelayernorm':\n",
        "          self.encoder = TFRobertaPreLayerNormModel(\n",
        "              RobertaPreLayerNormConfig(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  type_vocab_size = 3\n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        elif args == 'tfdebertav2':\n",
        "          self.encoder = TFDebertaV2Model(\n",
        "              DebertaV2Config(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  type_vocab_size = 3\n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(units=1024, name=\"fc\")\n",
        "        self.bn = keras.layers.BatchNormalization(name=\"bn\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.drop = keras.layers.Dropout(rate=0.4, name=\"drop\")\n",
        "\n",
        "        self.out = keras.layers.Dense(units=250, activation='softmax', name=\"out\")\n",
        "\n",
        "        self.xy_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.motion_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.hdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.pdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.oldist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.ildist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.content_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.fc.kernel_initializer = 'glorot_uniform'\n",
        "        self.out.kernel_initializer = 'glorot_uniform'\n",
        "\n",
        "    def get_att_mask(self, x):\n",
        "        att_mask = tf.math.reduce_sum(x, axis=-1)\n",
        "        att_mask = tf.cast(tf.math.not_equal(att_mask, 0), tf.float32)\n",
        "        return att_mask\n",
        "\n",
        "    def get_pool(self, x, x_mask):\n",
        "        x = x * tf.expand_dims(x_mask, axis=-1)  # apply mask\n",
        "        nonzero_count = tf.reduce_sum(x_mask, axis=1, keepdims=True)  # count nonzero elements\n",
        "        max_discount = (1-x_mask)*1e10\n",
        "\n",
        "        apool = tf.reduce_sum(x, axis=1) / nonzero_count\n",
        "        mpool = tf.reduce_max(x - tf.expand_dims(max_discount, axis=-1), axis=1)\n",
        "        spool = tf.sqrt((tf.reduce_sum(((x - tf.expand_dims(apool, axis=1)) ** 2) * tf.expand_dims(x_mask, axis=-1), axis=1) / nonzero_count) + 1e-9)\n",
        "        return tf.concat([apool, mpool, spool], axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        token_type_ids = tf.cast(x[:, :, -1], dtype = tf.int64)\n",
        "        hand_mask = x[:, :, -2]\n",
        "        x = x[:, :, :1196]\n",
        "        \n",
        "        x_mask = self.get_att_mask(x)\n",
        "\n",
        "        xy = self.xy_embeddings(x[:, :, :153])\n",
        "        motion = self.motion_embeddings(x[:, :, 153:306])\n",
        "        dist = self.hdist_embeddings(x[:, :, 306:516])\n",
        "        pdist = self.pdist_embeddings(x[:, :, 516:816])\n",
        "        oldist = self.oldist_embeddings(x[:, :, 816:1006])\n",
        "        ildist = self.ildist_embeddings(x[:, :, 1006:1196])\n",
        "\n",
        "        x = tf.concat([xy, motion, dist, pdist, oldist, ildist], axis=-1)\n",
        "        x = self.relu(x)\n",
        "        x = self.content_embeddings(x)\n",
        "        x = self.encoder(input_ids = None, inputs_embeds=x, attention_mask=x_mask, token_type_ids = token_type_ids).last_hidden_state\n",
        "\n",
        "        x = self.get_pool(x, hand_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CustomModelV2('tfrobertaprelayernorm')\n",
        "\n",
        "input_shape = (None, 1198)  # dynamic input shape\n",
        "\n",
        "# Create a model with an InputLayer to allow dynamic input shape\n",
        "inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qie4ZoYHvIqR",
        "outputId": "2f20efbd-b94d-463c-dc4b-ff8b7f6a2211"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 250) dtype=float32 (created by layer 'custom_model_v3')>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from transformers import TFRobertaPreLayerNormModel, TFDebertaV2Model, RobertaPreLayerNormConfig, DebertaV2Config\n",
        "\n",
        "class CustomModelV3(keras.Model):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV3, self).__init__()\n",
        "        \n",
        "        self.args = args\n",
        "        self.hidden = 300\n",
        "\n",
        "        self.xy_embeddings = keras.layers.Dense(units=self.hidden, name=\"xy_embeddings\")\n",
        "        self.motion_embeddings = keras.layers.Dense(units=self.hidden, name=\"motion_embeddings\")\n",
        "        self.hdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"hdist_embeddings\")\n",
        "        self.pdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"pdist_embeddings\")\n",
        "        self.oldist_embeddings = keras.layers.Dense(units=self.hidden, name=\"oldist_embeddings\")\n",
        "        self.ildist_embeddings = keras.layers.Dense(units=self.hidden, name=\"ildist_embeddings\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.content_embeddings = keras.layers.Dense(units=self.hidden, name=\"content_embeddings\")\n",
        "        \n",
        "        if args == 'tfrobertaprelayernorm':\n",
        "          self.encoder = TFRobertaPreLayerNormModel(\n",
        "              RobertaPreLayerNormConfig(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  type_vocab_size = 3\n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        elif args == 'tfdebertav2':\n",
        "          self.encoder = TFDebertaV2Model(\n",
        "              DebertaV2Config(\n",
        "                  hidden_size = self.hidden,\n",
        "                  num_hidden_layers = 1,\n",
        "                  num_attention_heads = 4,\n",
        "                  intermediate_size = 900,\n",
        "                  hidden_act = 'relu',\n",
        "                  vocab_size = 3, \n",
        "                  type_vocab_size = 3\n",
        "                  ),\n",
        "                  name=\"encoder\"\n",
        "                  )\n",
        "        \n",
        "        self.fc = keras.layers.Dense(units=1024, name=\"fc\")\n",
        "        self.bn = keras.layers.BatchNormalization(name=\"bn\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.drop = keras.layers.Dropout(rate=0.4, name=\"drop\")\n",
        "\n",
        "        self.out = keras.layers.Dense(units=250, activation='softmax', name=\"out\")\n",
        "\n",
        "        self.xy_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.motion_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.hdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.pdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.oldist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.ildist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.content_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.fc.kernel_initializer = 'glorot_uniform'\n",
        "        self.out.kernel_initializer = 'glorot_uniform'\n",
        "\n",
        "    def get_att_mask(self, x):\n",
        "        att_mask = tf.math.reduce_sum(x, axis=-1)\n",
        "        att_mask = tf.cast(tf.math.not_equal(att_mask, 0), tf.float32)\n",
        "        return att_mask\n",
        "\n",
        "    def get_pool(self, x, x_mask):\n",
        "        x = x * tf.expand_dims(x_mask, axis=-1)  # apply mask\n",
        "        nonzero_count = tf.reduce_sum(x_mask, axis=1, keepdims=True)  # count nonzero elements\n",
        "        max_discount = (1-x_mask)*1e10\n",
        "\n",
        "        apool = tf.reduce_sum(x, axis=1) / nonzero_count\n",
        "        mpool = tf.reduce_max(x - tf.expand_dims(max_discount, axis=-1), axis=1)\n",
        "        spool = tf.sqrt((tf.reduce_sum(((x - tf.expand_dims(apool, axis=1)) ** 2) * tf.expand_dims(x_mask, axis=-1), axis=1) / nonzero_count) + 1e-9)\n",
        "        return tf.concat([apool, mpool, spool], axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        token_type_ids = tf.cast(x[:, :, -1], dtype = tf.int64)\n",
        "        #hand_mask = x[:, :, -2]\n",
        "        x = x[:, :, :1196]\n",
        "        \n",
        "        x_mask = self.get_att_mask(x)\n",
        "\n",
        "        xy = self.xy_embeddings(x[:, :, :153])\n",
        "        motion = self.motion_embeddings(x[:, :, 153:306])\n",
        "        dist = self.hdist_embeddings(x[:, :, 306:516])\n",
        "        pdist = self.pdist_embeddings(x[:, :, 516:816])\n",
        "        oldist = self.oldist_embeddings(x[:, :, 816:1006])\n",
        "        ildist = self.ildist_embeddings(x[:, :, 1006:1196])\n",
        "\n",
        "        x = tf.concat([xy, motion, dist, pdist, oldist, ildist], axis=-1)\n",
        "        x = self.relu(x)\n",
        "        x = self.content_embeddings(x)\n",
        "        x = self.encoder(input_ids = None, inputs_embeds=x, attention_mask=x_mask, token_type_ids = token_type_ids).last_hidden_state\n",
        "\n",
        "        x = self.get_pool(x, x_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CustomModelV3('tfrobertaprelayernorm')\n",
        "\n",
        "input_shape = (None, 1198)  # dynamic input shape\n",
        "\n",
        "# Create a model with an InputLayer to allow dynamic input shape\n",
        "inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFRobertaPreLayerNormModel, TFDebertaV2Model, TFGPT2Model, RobertaPreLayerNormConfig, DebertaV2Config, GPT2Config\n",
        "\n",
        "class CustomModelV4(keras.Model):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV4, self).__init__()\n",
        "        \n",
        "        self.args = args\n",
        "        self.hidden = 512\n",
        "\n",
        "        self.xy_embeddings = keras.layers.Dense(units=self.hidden, name=\"xy_embeddings\")\n",
        "        self.motion_embeddings = keras.layers.Dense(units=self.hidden, name=\"motion_embeddings\")\n",
        "        self.hdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"hdist_embeddings\")\n",
        "        self.pdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"pdist_embeddings\")\n",
        "        self.oldist_embeddings = keras.layers.Dense(units=self.hidden, name=\"oldist_embeddings\")\n",
        "        self.ildist_embeddings = keras.layers.Dense(units=self.hidden, name=\"ildist_embeddings\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.content_embeddings = keras.layers.Dense(units=self.hidden, name=\"content_embeddings\")\n",
        "        \n",
        "        if args == 'mlp':\n",
        "          self.encoder = keras.layers.Dense(units=self.hidden, name=\"encoder\")\n",
        "        \n",
        "        self.fc = keras.layers.Dense(units=1024, name=\"fc\")\n",
        "        self.bn = keras.layers.BatchNormalization(name=\"bn\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.drop = keras.layers.Dropout(rate=0.4, name=\"drop\")\n",
        "\n",
        "        self.out = keras.layers.Dense(units=250, activation='softmax', name=\"out\")\n",
        "\n",
        "        self.xy_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.motion_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.hdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.pdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.oldist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.ildist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.content_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.encoder.kernel_initializer = 'glorot_uniform'\n",
        "        self.fc.kernel_initializer = 'glorot_uniform'\n",
        "        self.out.kernel_initializer = 'glorot_uniform'\n",
        "\n",
        "    def get_att_mask(self, x):\n",
        "        att_mask = tf.math.reduce_sum(x, axis=-1)\n",
        "        att_mask = tf.cast(tf.math.not_equal(att_mask, 0), tf.float32)\n",
        "        return att_mask\n",
        "\n",
        "    def get_pool(self, x, x_mask):\n",
        "        x = x * tf.expand_dims(x_mask, axis=-1)  # apply mask\n",
        "        nonzero_count = tf.reduce_sum(x_mask, axis=1, keepdims=True)  # count nonzero elements\n",
        "        max_discount = (1-x_mask)*1e10\n",
        "\n",
        "        apool = tf.reduce_sum(x, axis=1) / nonzero_count\n",
        "        mpool = tf.reduce_max(x - tf.expand_dims(max_discount, axis=-1), axis=1)\n",
        "        spool = tf.sqrt((tf.reduce_sum(((x - tf.expand_dims(apool, axis=1)) ** 2) * tf.expand_dims(x_mask, axis=-1), axis=1) / nonzero_count) + 1e-9)\n",
        "        return tf.concat([apool, mpool, spool], axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x_mask = self.get_att_mask(x)\n",
        "\n",
        "        xy = self.xy_embeddings(x[:, :, :153])\n",
        "        motion = self.motion_embeddings(x[:, :, 153:306])\n",
        "        dist = self.hdist_embeddings(x[:, :, 306:516])\n",
        "        pdist = self.pdist_embeddings(x[:, :, 516:816])\n",
        "        oldist = self.oldist_embeddings(x[:, :, 816:1006])\n",
        "        ildist = self.ildist_embeddings(x[:, :, 1006:1196])\n",
        "\n",
        "        x = tf.concat([xy, motion, dist, pdist, oldist, ildist], axis=-1)\n",
        "        x = self.relu(x)\n",
        "        x = self.content_embeddings(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        x = self.get_pool(x, x_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "model = CustomModelV4('mlp')\n",
        "\n",
        "input_shape = (None, 1196)  # dynamic input shape\n",
        "\n",
        "# Create a model with an InputLayer to allow dynamic input shape\n",
        "inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q638oC0_Q7V6",
        "outputId": "d6bdadd1-cf58-46d7-a594-21fe5fc4828c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 250) dtype=float32 (created by layer 'custom_model_v4')>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFRobertaPreLayerNormModel, TFDebertaV2Model, TFGPT2Model, RobertaPreLayerNormConfig, DebertaV2Config, GPT2Config\n",
        "\n",
        "class CustomModelV5(keras.Model):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV5, self).__init__()\n",
        "        \n",
        "        self.args = args\n",
        "        self.hidden = 384\n",
        "\n",
        "        self.xy_embeddings = keras.layers.Dense(units=self.hidden, name=\"xy_embeddings\")\n",
        "        self.motion_embeddings = keras.layers.Dense(units=self.hidden, name=\"motion_embeddings\")\n",
        "        self.hdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"hdist_embeddings\")\n",
        "        self.pdist_embeddings = keras.layers.Dense(units=self.hidden, name=\"pdist_embeddings\")\n",
        "        self.oldist_embeddings = keras.layers.Dense(units=self.hidden, name=\"oldist_embeddings\")\n",
        "        self.ildist_embeddings = keras.layers.Dense(units=self.hidden, name=\"ildist_embeddings\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.content_embeddings = keras.layers.Dense(units=self.hidden, name=\"content_embeddings\")\n",
        "        \n",
        "        if args == 'gru':\n",
        "          self.encoder = keras.layers.GRU(self.hidden, return_sequences=True, return_state=True)\n",
        "        \n",
        "        self.fc = keras.layers.Dense(units=1024, name=\"fc\")\n",
        "        self.bn = keras.layers.BatchNormalization(name=\"bn\")\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.drop = keras.layers.Dropout(rate=0.4, name=\"drop\")\n",
        "\n",
        "        self.out = keras.layers.Dense(units=250, activation='softmax', name=\"out\")\n",
        "\n",
        "        self.xy_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.motion_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.hdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.pdist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.oldist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.ildist_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.content_embeddings.kernel_initializer = 'glorot_uniform'\n",
        "        self.fc.kernel_initializer = 'glorot_uniform'\n",
        "        self.out.kernel_initializer = 'glorot_uniform'\n",
        "\n",
        "    def get_att_mask(self, x):\n",
        "        att_mask = tf.math.reduce_sum(x, axis=-1)\n",
        "        att_mask = tf.cast(tf.math.not_equal(att_mask, 0), tf.float32)\n",
        "        return att_mask\n",
        "\n",
        "    def get_pool(self, x, x_mask):\n",
        "        x = x * tf.expand_dims(x_mask, axis=-1)  # apply mask\n",
        "        nonzero_count = tf.reduce_sum(x_mask, axis=1, keepdims=True)  # count nonzero elements\n",
        "        max_discount = (1-x_mask)*1e10\n",
        "\n",
        "        apool = tf.reduce_sum(x, axis=1) / nonzero_count\n",
        "        mpool = tf.reduce_max(x - tf.expand_dims(max_discount, axis=-1), axis=1)\n",
        "        spool = tf.sqrt((tf.reduce_sum(((x - tf.expand_dims(apool, axis=1)) ** 2) * tf.expand_dims(x_mask, axis=-1), axis=1) / nonzero_count) + 1e-9)\n",
        "        return tf.concat([apool, mpool, spool], axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x_mask = self.get_att_mask(x)\n",
        "\n",
        "        xy = self.xy_embeddings(x[:, :, :153])\n",
        "        motion = self.motion_embeddings(x[:, :, 153:306])\n",
        "        dist = self.hdist_embeddings(x[:, :, 306:516])\n",
        "        pdist = self.pdist_embeddings(x[:, :, 516:816])\n",
        "        oldist = self.oldist_embeddings(x[:, :, 816:1006])\n",
        "        ildist = self.ildist_embeddings(x[:, :, 1006:1196])\n",
        "\n",
        "        x = tf.concat([xy, motion, dist, pdist, oldist, ildist], axis=-1)\n",
        "        x = self.relu(x)\n",
        "        x = self.content_embeddings(x)\n",
        "        x = self.relu(x)\n",
        "        x, _ = self.encoder(x)\n",
        "\n",
        "        x = self.get_pool(x, x_mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "model = CustomModelV5('gru')\n",
        "\n",
        "input_shape = (None, 1196)  # dynamic input shape\n",
        "\n",
        "# Create a model with an InputLayer to allow dynamic input shape\n",
        "inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejtQwBHWenQa",
        "outputId": "88327f18-a5c8-489a-d0e1-b2b4708a39f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 250) dtype=float32 (created by layer 'custom_model_v5')>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boUNy7KTFpyH"
      },
      "source": [
        "## scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tnrQLX0GNY5"
      },
      "outputs": [],
      "source": [
        "def lr_warmup_cosine_decay(global_step,\n",
        "                           warmup_steps,\n",
        "                           hold = 0,\n",
        "                           total_steps=0,\n",
        "                           start_lr=0.0,\n",
        "                           target_lr=1e-3):\n",
        "    # Cosine decay\n",
        "    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n",
        "\n",
        "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
        "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
        "\n",
        "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
        "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
        "    if hold > 0:\n",
        "        learning_rate = np.where(global_step > warmup_steps + hold,\n",
        "                                 learning_rate, target_lr)\n",
        "    \n",
        "    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
        "    return learning_rate\n",
        "\n",
        "class WarmupCosineDecay(keras.callbacks.Callback):\n",
        "    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n",
        "\n",
        "        super(WarmupCosineDecay, self).__init__()\n",
        "        self.start_lr = start_lr\n",
        "        self.hold = hold\n",
        "        self.total_steps = total_steps\n",
        "        self.global_step = 0\n",
        "        self.target_lr = target_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.lrs = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.global_step = self.global_step + 1\n",
        "        lr = self.model.optimizer.lr.numpy()\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n",
        "                                    total_steps=self.total_steps,\n",
        "                                    warmup_steps=self.warmup_steps,\n",
        "                                    start_lr=self.start_lr,\n",
        "                                    target_lr=self.target_lr,\n",
        "                                    hold=self.hold)\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "class TrainLoggerCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, log_file, model):\n",
        "        super().__init__()\n",
        "        self.log_file = log_file\n",
        "        self.model = model\n",
        "    \n",
        "    def on_train_begin(self, logs=None):\n",
        "        with open(self.log_file, 'a+') as f:\n",
        "            f.write(\"train start! \\n\")\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(f\"epoch : {epoch+1}, lr : {self.model.optimizer.lr.numpy()}, loss : {logs['loss']}, accuracy : {logs['accuracy']}, val_loss : {logs['val_categorical_crossentropy']}, val_accuracy : {logs['val_accuracy']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duwlR0AyFq8w"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEdycAkyGQnd"
      },
      "outputs": [],
      "source": [
        "def train(train_df, val_df, seed, path, model_name, version, epoch):\n",
        "  seed_everything(seed)\n",
        "  print('number of train data : ', len(train_df))\n",
        "  print('number of val data : ', len(val_df))\n",
        "  print('seed : ', seed)\n",
        "\n",
        "  num_train_data =len(train_df)\n",
        "  batch_size = 128\n",
        "  num_epochs = epoch\n",
        "  warmup_ratio = 0.2\n",
        "  lr = 1e-3\n",
        "  smoothing = 0.75\n",
        "  log_file = path + \"./log.txt\"\n",
        "\n",
        "  train_loader = Dataloader(args, train_df, data, batch_size, version, shuffle=True)\n",
        "  val_loader = Dataloader(args, val_df, data, batch_size, version, shuffle=True)\n",
        "\n",
        "  \n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "  weights_name = \"weights/epoch_{epoch:02d}-val_acc_{val_accuracy:.4f}.h5\"\n",
        "\n",
        "  checkpoint = ModelCheckpoint(path + weights_name, \n",
        "                              monitor='val_accuracy', \n",
        "                              verbose=1, \n",
        "                              save_weights_only=True, \n",
        "                              mode='max')\n",
        "                            \n",
        "  if version == 'v1':\n",
        "    model = CustomModel(model_name)\n",
        "    input_shape = (None, 1196) \n",
        "  elif version == 'v2':\n",
        "    model = CustomModelV2(model_name)\n",
        "    input_shape = (None, 1198) \n",
        "  elif version == 'v3':\n",
        "    model = CustomModelV3(model_name)\n",
        "    input_shape = (None, 1198) \n",
        "  elif version == 'v4':\n",
        "    model = CustomModelV4(model_name)\n",
        "    input_shape = (None, 1196) \n",
        "  elif version == 'v5':\n",
        "    model = CustomModelV5(model_name)\n",
        "    input_shape = (None, 1196) \n",
        "\n",
        "  inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "  model(inputs)\n",
        "\n",
        "  total_steps = (num_train_data // batch_size) * num_epochs\n",
        "  warmup_steps = int(warmup_ratio*total_steps)\n",
        "  print('total_steps: ', total_steps)\n",
        "  print('warmup_steps: ', warmup_steps)\n",
        "\n",
        "\n",
        "  callback = WarmupCosineDecay(total_steps=total_steps, \n",
        "                              warmup_steps=warmup_steps,\n",
        "                              hold=0, \n",
        "                              start_lr=0.0, \n",
        "                              target_lr=lr)\n",
        "\n",
        "  logger = TrainLoggerCallback(log_file, model)\n",
        "\n",
        "  optimizer = AdamW(learning_rate=lr)\n",
        "  loss = CategoricalCrossentropy(label_smoothing=smoothing, from_logits=False)\n",
        "  val_loss = CategoricalCrossentropy(label_smoothing=0.0, from_logits=False)\n",
        "\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=['accuracy', val_loss]\n",
        "  )\n",
        "\n",
        "\n",
        "  model.fit(\n",
        "      train_loader ,\n",
        "      validation_data = val_loader,\n",
        "      epochs = num_epochs,\n",
        "      batch_size = batch_size,\n",
        "      workers = 12, \n",
        "      verbose = 1,\n",
        "      callbacks = [checkpoint, callback, logger]\n",
        "  )\n",
        "\n",
        "  #del train_x\n",
        "  #del train_y\n",
        "  #del train_dataset\n",
        "  #del val_x\n",
        "  #del val_y\n",
        "  #del val_dataset\n",
        "  del model\n",
        "  gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWOsaH12tZwt"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2A-VtRPGR6N",
        "outputId": "e7081c4c-69dc-4cb9-ed08-29c9731bd6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  1 11:11:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    48W / 400W |    817MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PZDvrbTSsuAn"
      },
      "outputs": [],
      "source": [
        "# version 1\n",
        "folds, df = preprocess(args)\n",
        "i = 1\n",
        "train_df, val_df = df, folds[i][1][:1000]\n",
        "epoch = 40\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-robertaprelm-ls0.75-adamw-divide-head-emb300-v1/', \n",
        "      model_name = 'tfrobertaprelayernorm',\n",
        "      version = 'v1',\n",
        "      epoch = epoch)\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-debertav2-ls0.75-adamw-divide-head-emb300-v1/', \n",
        "      model_name = 'tfdebertav2',\n",
        "      version = 'v1',\n",
        "      epoch = epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPNh8CgrVvR8"
      },
      "outputs": [],
      "source": [
        "# version 2\n",
        "folds, df = preprocess(args)\n",
        "i = 1\n",
        "train_df, val_df = df, folds[i][1][:1000]\n",
        "epoch = 40\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-robertaprelm-ls0.75-adamw-divide-head-emb300-v2/', \n",
        "      model_name = 'tfrobertaprelayernorm',\n",
        "      version = 'v2',\n",
        "      epoch = epoch)\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-debertav2-ls0.75-adamw-divide-head-emb300-v2/', \n",
        "      model_name = 'tfdebertav2',\n",
        "      version = 'v2',\n",
        "      epoch = epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpiyhpH2wGQv"
      },
      "outputs": [],
      "source": [
        "# version 3\n",
        "folds, df = preprocess(args)\n",
        "i = 1\n",
        "train_df, val_df = df, folds[i][1][:1000]\n",
        "epoch = 40\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-robertaprelm-ls0.75-adamw-divide-head-emb300-v3/', \n",
        "      model_name = 'tfrobertaprelayernorm',\n",
        "      version = 'v3',\n",
        "      epoch = epoch)\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/version-ensemble/keras-debertav2-ls0.75-adamw-divide-head-emb300-v3/', \n",
        "      model_name = 'tfdebertav2',\n",
        "      version = 'v3',\n",
        "      epoch = epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4\n",
        "folds, df = preprocess(args)\n",
        "i = 1\n",
        "train_df, val_df = df, folds[i][1][:1000]\n",
        "epoch = 40\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/keras-mlp-ls0.75-adamw-divide-head-emb512/', \n",
        "      model_name = 'mlp',\n",
        "      version = 'v4',\n",
        "      epoch = epoch)"
      ],
      "metadata": {
        "id": "zPFsx3PwRywE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 5\n",
        "folds, df = preprocess(args)\n",
        "i = 1\n",
        "train_df, val_df = df, folds[i][1][:1000]\n",
        "epoch = 40\n",
        "\n",
        "train(train_df = train_df, \n",
        "      val_df = val_df,\n",
        "      seed = 1,\n",
        "      path = f'/content/drive/MyDrive/Kaggle/model/keras-gru-ls0.75-adamw-divide-head-emb384/', \n",
        "      model_name = 'gru',\n",
        "      version = 'v5',\n",
        "      epoch = epoch)"
      ],
      "metadata": {
        "id": "f6zQ4yjYf-Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "2dWqt_0NAHaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RU43IlEiFTWj",
        "LYpYwszTFXCO",
        "v5vrK7McFfNH",
        "0JTuuQTFFgSU",
        "K8VdKhkkFhYa",
        "Gl6di3bCFis4",
        "pzBU3HhGFkB3",
        "A2XW6GYiFlw7",
        "9bdAI5YxFncb",
        "IXbHRIskFoeK",
        "boUNy7KTFpyH",
        "duwlR0AyFq8w"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}